CM- gets heartbeats from pods, node monitor period is 5 sec(node monitor period)
grace period is 40s meaning it will wait 40seconds until it marks node as unhealthy and waits 5 second again before removing it
scheduler schedules using 2 thibngs:
it filters nodes
it ranks nodes

kubectk run nginx --image ngunx
kubectl replace -f replica.yaml
kubectk scale --replicas=6 replica.yaml or nameof RS
kubectl get pods --namespace=kubesystem
kubectl get pods --all-namespaces -o wide 
kubecrl create namespace dev
resoyrce quota yaml file limits resource utilisation like cpu etc to its own NS

7 / 7
What DNS name should the Blue application use to access the database db-service in the dev namespace?
db-service.dev.svc.cluster.local
kubectl expose pod redis --port=6379 --name redis-service
kubectl create deployment  webapp --image=kodekloud/webapp-color --replicas=3 -n=mynamespace

kubectl run httpd --image=httpd:alpine --port=80 --expose
edit files:
kubectl replace commands
kubectl get pods --selector apps=myapp

what happens if scheduler isnt there?
All pods continue to be in pending state thus we can use
 nodename:node02 component in the file while creating yaml file
if the pod is already created and you want to assign it to a pod it can be done with the help of a curl command using post request

Taints and tolerations:
taints are of 3 typeS:
no schedule
no execute
preferred no schedule (use it when there are some performance issues on nodes, consider this node as a last option)
eg: kubectl taint nodes nodename key1=value1:NoSchedule

is useful in upgrading to a newer version, where we make a node unschedulable and then upgrade the node 
stop a node from getting pod assigned to it
only the pod that has tolerations assigned to it will be scheduled on that node
so be default a taint will be applied on master so we dont see the scheduling here



kubectl taint nodes node01 spray=mortein:NoSchedule
so inorder to identify the node, we add a label to the node

node selectors: select the nodes to place the pods on 
the node selector attribute is added to the deployment yml file : this can cause scheduling issues if node is not available
we do it by specifiying lables in the yaml file of the node
this can be useful when we need to run the appln on a node having windows installed on it or some kind of arm processors installed on them.

node affinity:
1)preffered-->if you find the label schedule it or else its ok
2)required-->same as node selectors


kubectl label nodes node-1 size=large (this way we mark the largest node ) and we define the node selectors as size:Large in deployement file



kubectl apply:
creates live object config file once the apply command in run and also a json file is created called last object config


Can k9 function without master plane?
yes: we can store the yaml files in a directory and kubelet reads info from here 
this directory can be /etc/k8/manifests from which it is destined to take pod info.
THese are called statis pods which are created by the kubelet without the intervention of the master plane but we can only create pods and not deployment or services, we will get the path in the pod manifest file


K8 can have multiple scheduler
-> it must have diff names

(SFSB)
Pods also have priorities assigned to them, so a pod comes and waits in the scheduling queue and the highest priority one is picked first
then it goes to filter phase and nodes which cannot schedule this pod are filteed out
then comes the scoring phase- the nodes are scored on the cpu/memory that would be left after the pods are scheduled.
next is binding phase and pods are binded to node
This process is done by scheduling plugins

kubectl logs -f webapp-1
kubectl logs -f podname containername


Rolling updates:
when we first create a deployment, it creates a rollout-> new rollout creates -->new revision--> 
then we upgrade the container-->new rollout->new reviosn

kubectl rollout status deploymentname
kubectl rollout history deploymentname
kubectl rollout undo deploymentname
kubectl describe deployment (get deploymemt statergy)
kubectl edit deployment frontend

different deployment stratergies:
1)Recreate:
destroy existing instances of all aplln and create a new one but we need appln downtime
when we apply the replicaset will be taken down to 0 and then again comes back to the specified no.

2)Rolling update:
we take down older version and bring new one, one by one (default)


using args command in .yaml file eg: args: ["10"]
it overrides the cmd command of the docker 
using "command" overrides the entrypoint command of the docker

always take the env variables from the configmap file and add an additional attribute called envfrom in the pod definition file of deployment

"spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color"

 kubectl get configmaps

secrets-->
echo -n "secrettoencrypt' | base64
echo -n "secrettoencrypt' | base64 --decode
*************************************************************************************************************************
Security: controlling access to api-server ( access )
authentication: token, user id -pw, ldap, 
rbac (role based access)
pods can access other pods inside cluster

steps: kubeapi server authenticates: we can give authentication through ldap, or static files like below
create a csv file containing username,id and password using basic auth file, this can conatin group also
alrenative is using a token auth file instaed of pw file

Certificates: necessat btw 2 parties as a source of trust for any kind of activities
eg: makes sure communication btw user and web service is encrytpted
we encrypt the data and send a key seperatley to decrypt it-->symmentric encryption
pvt key and public key is used seperately-->assym 



Authorization:
RBAC,Node,ABAC,wenhook


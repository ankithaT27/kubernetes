CM- gets heartbeats from pods, node monitor period is 5 sec(node monitor period)
grace period is 40s meaning it will wait 40seconds until it marks node as unhealthy and waits 5 second again before removing it
scheduler schedules using 2 thibngs:
it filters nodes
it ranks nodes

kubectk run nginx --image ngunx
kubectl replace -f replica.yaml
kubectk scale --replicas=6 replica.yaml or nameof RS
kubectl get pods --namespace=kubesystem
kubectl get pods --all-namespaces -o wide 
kubecrl create namespace dev
resoyrce quota yaml file limits resource utilisation like cpu etc to its own NS

7 / 7
What DNS name should the Blue application use to access the database db-service in the dev namespace?
db-service.dev.svc.cluster.local
kubectl expose pod redis --port=6379 --name redis-service
kubectl create deployment  webapp --image=kodekloud/webapp-color --replicas=3 -n=mynamespace

kubectl run httpd --image=httpd:alpine --port=80 --expose
edit files:
kubectl replace commands
kubectl get pods --selector apps=myapp

what happens if scheduler isnt there?
All pods continue to be in pending state thus we can use
 nodename:node02 component in the file while creating yaml file
if the pod is already created and you want to assign it to a pod it can be done with the help of a curl command using post request

Taints and tolerations:
taints are of 3 typeS:
no schedule
no execute
preferred no schedule (use it when there are some performance issues on nodes, consider this node as a last option)
eg: kubectl taint nodes nodename key1=value1:NoSchedule

is useful in upgrading to a newer version, where we make a node unschedulable and then upgrade the node 
stop a node from getting pod assigned to it
only the pod that has tolerations assigned to it will be scheduled on that node
so be default a taint will be applied on master so we dont see the scheduling here



kubectl taint nodes node01 spray=mortein:NoSchedule
so inorder to identify the node, we add a label to the node

node selectors: select the nodes to place the pods on 
the node selector attribute is added to the deployment yml file : this can cause scheduling issues if node is not available
we do it by specifiying lables in the yaml file of the node
this can be useful when we need to run the appln on a node having windows installed on it or some kind of arm processors installed on them.

There are two types of node affinity:

RequiredDuringSchedulingIgnoredDuringExecution: This specifies that the pod won't be scheduled on a node unless it satisfies the affinity rules. However, if the node doesn't match the affinity rules during pod execution (e.g., if labels on the node change), the pod will still continue to run on that node.

PreferredDuringSchedulingIgnoredDuringExecution: This expresses a preference for scheduling a pod onto a node with the specified labels, but it's not a hard requirement. If no node matches the affinity rules, the pod will still be scheduled. However, nodes that match the affinity rules will be given a higher priority during scheduling.

Node affinity can be defined based on node labels or node affinity expressions. Here's an example YAML snippet demonstrating node affinity based on node labels:

Node affinity can be particularly useful for scenarios such as ensuring that certain pods are co-located with specific hardware, ensuring data locality, or separating workloads onto different nodes for isolation.

node affinity:
1)preffered-->if you find the label schedule it or else its ok
2)required-->same as node selectors


kubectl label nodes node-1 size=large (this way we mark the largest node ) and we define the node selectors as size:Large in deployement file



kubectl apply:
creates live object config file once the apply command in run and also a json file is created called last object config


Can k9 function without master plane?
yes: we can store the yaml files in a directory and kubelet reads info from here 
this directory can be /etc/k8/manifests from which it is destined to take pod info.
THese are called statis pods which are created by the kubelet without the intervention of the master plane but we can only create pods and not deployment or services, we will get the path in the pod manifest file
Static pods in Kubernetes are a way to run pods directly on a node without the involvement of the Kubernetes control plane. Unlike regular pods managed by the Kubernetes API server, static pods are managed directly by the Kubelet daemon on a specific node.
Static pods are not managed by the Kubernetes API server. Instead, they are detected and managed by the Kubelet running on the node where the pods are supposed to run.


K8 can have multiple scheduler
-> it must have diff names

(SFSB)
Pods also have priorities assigned to them, so a pod comes and waits in the scheduling queue and the highest priority one is picked first
then it goes to filter phase and nodes which cannot schedule this pod are filteed out
then comes the scoring phase- the nodes are scored on the cpu/memory that would be left after the pods are scheduled.
next is binding phase and pods are binded to node
This process is done by scheduling plugins

kubectl logs -f webapp-1
kubectl logs -f podname containername


Rolling updates:
when we first create a deployment, it creates a rollout-> new rollout creates -->new revision--> 
then we upgrade the container-->new rollout->new reviosn

kubectl rollout status deploymentname
kubectl rollout history deploymentname
kubectl rollout undo deploymentname
kubectl describe deployment (get deploymemt statergy)
kubectl edit deployment frontend

different deployment stratergies:
1)Recreate:
destroy existing instances of all aplln and create a new one but we need appln downtime
when we apply the replicaset will be taken down to 0 and then again comes back to the specified no.

2)Rolling update:
we take down older version and bring new one, one by one (default)

kubectl rollout undo


using args command in .yaml file eg: args: ["10"]
it overrides the cmd command of the docker 
using "command" overrides the entrypoint command of the docker

always take the env variables from the configmap file and add an additional attribute called envfrom in the pod definition file of deployment

You can mount a Secret or ConfigMap as a volume inside your pod, making its data available to the containers running in the pod.

We specify an environment variable DATABASE_URL.
We use the valueFrom field with configMapKeyRef to reference the database_url key from the ConfigMap named my-configmap.

"spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color"

 kubectl get configmaps

secrets-->
echo -n "secrettoencrypt' | base64
echo -n "secrettoencrypt' | base64 --decode
*************************************************************************************************************************
multi container pods:
share the same lifecycle, share same volumes, nw etc
kubectl logs <pod-name> -c <container-name>]
kubectl exec -it <pod-name> [-c <container-name>] -- <command>

init containers:
Init containers provide a way to ensure that necessary setup tasks are completed before the main application starts, improving reliability and ensuring that the application has everything it needs to run properly.
Init containers in Kubernetes are specialized containers that run and complete before any other containers in a pod start. They are primarily used to perform setup tasks such as initializing configurations, downloading necessary files, or waiting for external resources to become available before the main application containers start
Init containers provide a way to ensure that necessary setup tasks are completed before the main application starts, improving reliability and ensuring that the application has everything it needs to run properly.

**********************************************************************************************************************

kubectl drain <node-name>
kubectl cordon <node-name> (cordoning makes it unschedulable, uncordon to schedule again)
kubectl drain node01 --ignore-daemonsets (to ignore)

you will see that there is a single pod scheduled on node01 which is not part of a replicaset.

The drain command will not work in this case. To forcefully drain the node we now have to use the --force flag.



Draining a node in Kubernetes is a process used to safely remove the node from the cluster. It involves evicting all the pods running on the node and preventing new pods from being scheduled onto it. This is typically done when performing 
maintenance on the node, such as upgrading the Kubernetes version, rebooting the node, or decommissioning it.



***************************************************************************************************************************

v12.02.33
major,minor,patch
Security: controlling access to api-server ( access )
authentication: token, user id -pw, ldap, 
rbac (role based access)
pods can access other pods inside cluster

***********************************************************************************************************************

cluster upgrades:

kube api-server shouls always have a hugher version, CM and scheudler should be v-1 and kubelet and kube-proxy should be v-2
kubectl can be v+1,v or v-1
k8 supports last 2 versions from its recent release only
upgrade 1:
 -> upgrade master first and take downtime to upgrade all other nodes
upgrade type 2:
->  upgrade nodes one at a time
upgrade type 3:
-> just add new nodes and decom old

kubeadm upgrade plan ->command gives all info reg upgrade
upgrade kubeadm--> apt-get update kubeadm version=
kubeadm upgrade apply v12.33.0

*************************************************************************************************

backup and restore

take backups of yaml files in github
take etcd backup 
kubectl get all --namespace=kube-system (get infor reg kube components)
kubectl logs etcd-controlplane -n kube-system
kubectl -n alpha get all





**************************************************************************************************************************

image security
docker login private-reg.io
docker run private-reg.io/ankitha2t7/imagename
kubectl create secret docker-registry 
kubectl create secret docker-registry my-docker-secret --docker-server=your.registry.example.com --docker-username=your-username --docker-password=your-password --docker-email=your-email@example.com

kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com

later add a key-value in depl.yaml file to fetch this secret:

imagepullsecret: private-reg-cred


steps: kubeapi server authenticates: we can give authentication through ldap, or static files like below
create a csv file containing username,id and password using basic auth file, this can conatin group also
alrenative is using a token auth file instaed of pw file

Certificates: necessat btw 2 parties as a source of trust for any kind of activities
eg: makes sure communication btw user and web service is encrytpted
we encrypt the data and send a key seperatley to decrypt it-->symmentric encryption
pvt key and public key is used seperately-->assym 




Authorization:
RBAC,Node,ABAC,wenhook

Nw policies:
we can add a nw policy on a pod to accept traffic from a particular pod on port 3306




